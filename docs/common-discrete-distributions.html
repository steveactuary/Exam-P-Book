<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Common Discrete Distributions | An Exam P Study Guide</title>
  <meta name="description" content="This is a study guide for exam P." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Common Discrete Distributions | An Exam P Study Guide" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a study guide for exam P." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Common Discrete Distributions | An Exam P Study Guide" />
  
  <meta name="twitter:description" content="This is a study guide for exam P." />
  

<meta name="author" content="Actuary Helper" />


<meta name="date" content="2020-01-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="functions-of-discrete-random-variables.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<script src="libs/elevate-section-attrs-2.0/elevate-section-attrs.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> An Introduction to Set Theory</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#defining-sets"><i class="fa fa-check"></i><b>1.1</b> Defining Sets</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#set-equality-and-subsets"><i class="fa fa-check"></i><b>1.2</b> Set Equality and Subsets</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#set-operations-and-venn-diagrams"><i class="fa fa-check"></i><b>1.3</b> Set Operations and Venn Diagrams</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#venn-diagram"><i class="fa fa-check"></i><b>1.3.1</b> Venn Diagram</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#union"><i class="fa fa-check"></i><b>1.3.2</b> Union</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#intersection"><i class="fa fa-check"></i><b>1.3.3</b> Intersection</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#complement"><i class="fa fa-check"></i><b>1.3.4</b> Complement</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#set-difference"><i class="fa fa-check"></i><b>1.3.5</b> Set Difference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#special-sets-and-identities"><i class="fa fa-check"></i><b>1.4</b> Special Sets and Identities</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#sample-space-and-empty-set"><i class="fa fa-check"></i><b>1.4.1</b> Sample Space and Empty Set</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#de-morgans-laws"><i class="fa fa-check"></i><b>1.4.2</b> De Morgan’s Laws</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#or-more-sets"><i class="fa fa-check"></i><b>1.4.3</b> 3 or More Sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="counting.html"><a href="counting.html"><i class="fa fa-check"></i><b>2</b> Counting</a>
<ul>
<li class="chapter" data-level="2.1" data-path="counting.html"><a href="counting.html#size-of-a-set"><i class="fa fa-check"></i><b>2.1</b> Size of a set</a></li>
<li class="chapter" data-level="2.2" data-path="counting.html"><a href="counting.html#principle-of-inclusion-exclusion"><i class="fa fa-check"></i><b>2.2</b> Principle of Inclusion-Exclusion</a></li>
<li class="chapter" data-level="2.3" data-path="counting.html"><a href="counting.html#multiplication-principle"><i class="fa fa-check"></i><b>2.3</b> Multiplication Principle</a></li>
<li class="chapter" data-level="2.4" data-path="counting.html"><a href="counting.html#permutations"><i class="fa fa-check"></i><b>2.4</b> Permutations</a></li>
<li class="chapter" data-level="2.5" data-path="counting.html"><a href="counting.html#combinations"><i class="fa fa-check"></i><b>2.5</b> Combinations</a></li>
<li class="chapter" data-level="2.6" data-path="counting.html"><a href="counting.html#variations"><i class="fa fa-check"></i><b>2.6</b> Variations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-basics.html"><a href="probability-basics.html"><i class="fa fa-check"></i><b>3</b> Probability Basics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-basics.html"><a href="probability-basics.html#equiprob"><i class="fa fa-check"></i><b>3.1</b> Equally Likely Events</a></li>
<li class="chapter" data-level="3.2" data-path="probability-basics.html"><a href="probability-basics.html#terminology"><i class="fa fa-check"></i><b>3.2</b> Terminology</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-basics.html"><a href="probability-basics.html#sample-spaces-and-events"><i class="fa fa-check"></i><b>3.2.1</b> Sample Spaces and Events</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-basics.html"><a href="probability-basics.html#probability-functions"><i class="fa fa-check"></i><b>3.3</b> Probability Functions</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability-basics.html"><a href="probability-basics.html#axiomsprob"><i class="fa fa-check"></i><b>3.3.1</b> Probability Axioms</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-basics.html"><a href="probability-basics.html#conditional-probability-and-independence"><i class="fa fa-check"></i><b>3.4</b> Conditional Probability and Independence</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="probability-basics.html"><a href="probability-basics.html#condprob"><i class="fa fa-check"></i><b>3.4.1</b> Conditional Probability Formulas</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-basics.html"><a href="probability-basics.html#independence"><i class="fa fa-check"></i><b>3.4.2</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="probability-basics.html"><a href="probability-basics.html#bayes-theorem"><i class="fa fa-check"></i><b>3.5</b> Bayes Theorem</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="probability-basics.html"><a href="probability-basics.html#bayes-theorem-intuition"><i class="fa fa-check"></i><b>3.5.1</b> Bayes Theorem Intuition</a></li>
<li class="chapter" data-level="3.5.2" data-path="probability-basics.html"><a href="probability-basics.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.5.2</b> Law of Total Probability</a></li>
<li class="chapter" data-level="3.5.3" data-path="probability-basics.html"><a href="probability-basics.html#bayes-theorem-formula"><i class="fa fa-check"></i><b>3.5.3</b> Bayes Theorem Formula</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#what-is-a-random-variable"><i class="fa fa-check"></i><b>4.1</b> What is a Random Variable</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#probability-distribution-functions"><i class="fa fa-check"></i><b>4.2</b> Probability Distribution Functions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="random-variables.html"><a href="random-variables.html#probability-of-an-outcome"><i class="fa fa-check"></i><b>4.2.1</b> Probability of an Outcome</a></li>
<li class="chapter" data-level="4.2.2" data-path="random-variables.html"><a href="random-variables.html#probability-distribution-function-definition"><i class="fa fa-check"></i><b>4.2.2</b> Probability Distribution Function Definition</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="random-variables.html"><a href="random-variables.html#cdf"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Functions</a></li>
<li class="chapter" data-level="4.4" data-path="random-variables.html"><a href="random-variables.html#discrete-vs.-continuous-random-variables"><i class="fa fa-check"></i><b>4.4</b> Discrete vs. Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="random-variables.html"><a href="random-variables.html#a-bit-more-precise"><i class="fa fa-check"></i><b>4.4.1</b> A Bit More Precise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html"><i class="fa fa-check"></i><b>5</b> Functions of Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#what-is-the-mean"><i class="fa fa-check"></i><b>5.1</b> What is the Mean?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#eax-b"><i class="fa fa-check"></i><b>5.1.1</b> E(aX + b)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#what-is-variance"><i class="fa fa-check"></i><b>5.2</b> What is Variance?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#defining-variance"><i class="fa fa-check"></i><b>5.2.1</b> Defining Variance</a></li>
<li class="chapter" data-level="5.2.2" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#variance-intuition"><i class="fa fa-check"></i><b>5.2.2</b> Variance Intuition</a></li>
<li class="chapter" data-level="5.2.3" data-path="functions-of-discrete-random-variables.html"><a href="functions-of-discrete-random-variables.html#vaxb"><i class="fa fa-check"></i><b>5.2.3</b> V(aX+b)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html"><i class="fa fa-check"></i><b>6</b> Common Discrete Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>6.1</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="6.2" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>6.2</b> Binomial Distribution</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Mean and Variance of the Binomial Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>6.3</b> Hypergeometric Distribution</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#hypergeometric-vs.-binomial"><i class="fa fa-check"></i><b>6.3.1</b> Hypergeometric vs. Binomial</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>6.4</b> Poisson Distribution</a></li>
<li class="chapter" data-level="6.5" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>6.5</b> Geometric Distribution</a></li>
<li class="chapter" data-level="6.6" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>6.6</b> Negative Binomial Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="common-discrete-distributions.html"><a href="common-discrete-distributions.html#other-more-common-formulation"><i class="fa fa-check"></i><b>6.6.1</b> Other (more common) formulation</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Exam P Study Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="common-discrete-distributions" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Common Discrete Distributions</h1>
<div id="bernoulli-distribution" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Bernoulli Distribution</h2>
<p>The Bernoulli distribution is appropriate when there are two outcomes. Success happens with probability <span class="math inline">\(p\)</span> and failure happens with probability <span class="math inline">\(1-p\)</span>.
<span class="math display" id="eq:bernoulli">\[\begin{equation*} 
    p(x) =
    \left\{
        \begin{array}{cc}
                p &amp; \mathrm{for\ } x=1 \\
                1-p &amp; \mathrm{for\ } x=0 \\
        \end{array} 
    \right.
\tag{1}
\end{equation*}\]</span></p>
<hr />

<div class="exercise">
<span id="exr:bernoulli" class="exercise"><strong>Exercise 3  </strong></span>Let <span class="math inline">\(X\)</span> be a Bernoulli random variable. Show that <span class="math inline">\(E(X)=p\)</span> and that <span class="math inline">\(V(X)=p \cdot (1-p)\)</span>.
</div>
<hr />
<p>The number of heads from a single coin flip is an example of a Bernoulli random variable. The event that the coin lands on heads is success, tails is failure. If the coin is fair, <span class="math inline">\(p\)</span> = .5.</p>
</div>
<div id="binomial-distribution" class="section level2" number="6.2">
<h2 number="6.2"><span class="header-section-number">6.2</span> Binomial Distribution</h2>
<p>We have an unfair coin that is heads <span class="math inline">\(60\%\)</span> of the time and tails <span class="math inline">\(40\%\)</span> of the time. We flip the coin <span class="math inline">\(5\)</span> times. We express the number of heads as the sum of Bernoulli random variables, <span class="math inline">\(X = H_1 + H_2 + H_3 + H_4 + H_5\)</span> where
<span class="math display">\[\begin{equation*} 
    H_i =
    \left\{
        \begin{array}{cc}
                1 \text{ if flip } i \text{ is heads} \\
                0 \text{ if flip } i \text{ is tails} \\
        \end{array} 
    \right.
\end{equation*}\]</span></p>
<p>We wish to calculate the probability that all <span class="math inline">\(5\)</span> flips are heads. Because coin flips are independent we can use the rule <span class="math inline">\(P(A \cap B) = P(A) \cdot P(B)\)</span>
<span class="math display">\[\begin{align} 
P(X=5) &amp;= P((H_1=1) \cap (H_2=1) \cap (H_3=1) \cap (H_4=1) \cap (H_5=1)) \notag \\
&amp;=P(H_1=1) \cdot P(H_2=1) \cdot P(H_3=1) \cdot P(H_4=1) \cdot P(H_5=1)  \notag \\
&amp;= .6^5 \notag
\end{align}\]</span></p>
<p>Less formally, the only way to achieve all heads is to flip the sequence <span class="math inline">\(HHHHH\)</span>. Each head happens with probability <span class="math inline">\(.6\)</span> and the answer is <span class="math inline">\(.6^5\)</span>.</p>
<p>What about the probability of flipping exactly <span class="math inline">\(4\)</span> heads? The sequences that can lead to this outcome are:
<span class="math display">\[\begin{align} 
THHHH \notag \\
HTHHH  \notag \\
HHTHH \notag \\
HHHTH \notag \\
HHHHT \notag
\end{align}\]</span></p>
<p>These sequences all represent distinct outcomes for <span class="math inline">\(X\)</span>. Although some of the sequences are the same in some places, each represents a distinct outcome of <span class="math inline">\(X\)</span>, disjoint from all other outcomes. So we can use the addition formula for the probability of disjoint events.
<span class="math display">\[\begin{align} 
&amp;P(X=4) = \notag \\
&amp;P((THHHH) \cup (HTHHH) \cup (HHTHH) \cup (HHHTH) \cup (HHHHT)) = \notag \\
&amp;P(THHHH) + P(HTHHH) + P(HHTHH) + P(HHHTH) + P(HHHHT)=  \notag \\
&amp;5 \cdot .6^4 \cdot .4 \notag
\end{align}\]</span></p>
<p>What about the probability of <span class="math inline">\(3\)</span> heads? There are many sequences of flips that result in this, like <span class="math inline">\(HTHTH\)</span>. Each of these sequences will have probability <span class="math inline">\(.6^3 \cdot .4^2\)</span> because there are three heads and two tails. To find out how many sequences there are we must answer the question “How many ways are there to assign three of the five coins to be heads?” The answer to this question is:
<span class="math display">\[{5 \choose 3} = \frac{5!}{3!2!} = 10\]</span>
There are <span class="math inline">\({5 \choose 3}\)</span> ways to flip <span class="math inline">\(3\)</span> heads and each way has probability <span class="math inline">\(.6^3 \cdot .4^2\)</span>, so the probability of three heads is <span class="math inline">\({5 \choose 3} \cdot .6^3 \cdot .4^2\)</span>.</p>
<p>This technique works for the other calculations we have done. There are <span class="math inline">\({5 \choose 5} = \frac{5!}{5!0!}=1\)</span> ways to flip <span class="math inline">\(5\)</span> heads and <span class="math inline">\({5 \choose 4} = \frac{5!}{4!1!}=5\)</span> ways to flip <span class="math inline">\(4\)</span> heads.</p>
<p>Let’s generalize from our coin flipping. We had <span class="math inline">\(5\)</span> independent Bernoulli trials where each Bernoulli trial had a probability of success as <span class="math inline">\(.6\)</span>. What if we had <span class="math inline">\(n\)</span> Bernoulli trials each with a probability of success of <span class="math inline">\(p\)</span>. This is called the <strong>binomial distribution</strong> with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.</p>
<hr />

<div class="theorem">
<span id="thm:unnamed-chunk-20" class="theorem"><strong>Theorem 3  (Binomial Distribution)  </strong></span>Let <span class="math inline">\(X\)</span> have the binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. We have:
<span class="math display">\[p(k) = P(X=k) = {n \choose k}p^k(1-p)^{n-k}\]</span>
</div>
<hr />
<p>Be sure that this makes perfect sense.</p>
<div id="mean-and-variance-of-the-binomial-distribution" class="section level3" number="6.2.1">
<h3 number="6.2.1"><span class="header-section-number">6.2.1</span> Mean and Variance of the Binomial Distribution</h3>
<p>In exercise <a href="common-discrete-distributions.html#exr:bernoulli">3</a> the motivated student showed that if <span class="math inline">\(X\)</span> is Bernoulli with parameter <span class="math inline">\(p\)</span> then <span class="math inline">\(E(X) = p\)</span> and <span class="math inline">\(V(X) = p(1-p)\)</span>. These formulas for the binomial distribution are very similar.</p>
<hr />

<div class="theorem">
<span id="thm:unnamed-chunk-21" class="theorem"><strong>Theorem 4  (Binomial Distribution)  </strong></span>Let <span class="math inline">\(X\)</span> have the binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. We have:
<span class="math display">\[E(X) = np\]</span>
<span class="math display">\[V(X) = np(1-p)\]</span>
</div>
<hr />
<div id="intuition" class="section level4" number="6.2.1.1">
<h4 number="6.2.1.1"><span class="header-section-number">6.2.1.1</span> Intuition</h4>
<p>Let <span class="math inline">\(X\)</span> be the number of heads from a single coin flip. Let <span class="math inline">\(Y\)</span> be the number of heads from <span class="math inline">\(10\)</span> coin flips.
<span class="math display">\[E(X)=np=1 \cdot .5\]</span>
<span class="math display">\[E(Y)=np=10 \cdot .5\]</span></p>
<p>Think about how many heads you expect to get when you flip <span class="math inline">\(10\)</span> coins. It seems like it should be <span class="math inline">\(10\)</span> times the expected number of heads from <span class="math inline">\(1\)</span> coin. Apparently the variance is also <span class="math inline">\(10\)</span> times larger for <span class="math inline">\(10\)</span> times the coins.</p>
<p><span class="math display">\[V(X)=np(1-p)=1 \cdot .5^2\]</span>
<span class="math display">\[V(Y)=np(1-p)=10 \cdot .5^2\]</span></p>
</div>
</div>
</div>
<div id="hypergeometric-distribution" class="section level2" number="6.3">
<h2 number="6.3"><span class="header-section-number">6.3</span> Hypergeometric Distribution</h2>
<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-22" class="example"><strong>Example 5  (Hypergeometric Rocks)  </strong></span>We have <span class="math inline">\(15\)</span> rocks in a bag. <span class="math inline">\(7\)</span> rocks are red and <span class="math inline">\(8\)</span> are black. We select <span class="math inline">\(5\)</span> rocks. What is the probability of selecting exactly <span class="math inline">\(2\)</span> red rocks and <span class="math inline">\(3\)</span> black rocks.</p>
<p>Any selection of rocks is equally likely. We can use our formula for the probability when all events are equally likely.</p>
<p><span class="math display">\[\frac{\text{Number of Selected Outcomes}}{\text{Total Possible Outcomes}}\]</span></p>
<p>For the denominator we need to find how many ways there are to draw <span class="math inline">\(5\)</span> rocks from a set of <span class="math inline">\(15\)</span>. This is <span class="math inline">\(15 \choose 5\)</span>.</p>
<p>For the numerator we need to select <span class="math inline">\(2\)</span> red rocks from a set of <span class="math inline">\(7\)</span>. This can be done in <span class="math inline">\(7 \choose 2\)</span> ways. We can select <span class="math inline">\(3\)</span> blue rocks from <span class="math inline">\(8\)</span> in <span class="math inline">\(8 \choose 3\)</span> ways. There are then <span class="math inline">\({7 \choose 2}{8 \choose 3}\)</span> ways to select the rocks.</p>
The answer is then:
<span class="math display">\[\large \frac{{7 \choose 2}{8 \choose 3}}{{15 \choose 5}}\]</span>
</div>
<hr />
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-23" class="exercise"><strong>Exercise 4  (Hypergeometric Cards)  </strong></span>We have a deck of <span class="math inline">\(40\)</span> cards. <span class="math inline">\(30\)</span> cards are red and <span class="math inline">\(10\)</span> are black. We draw a hand of <span class="math inline">\(5\)</span> cards. Show that the probability of drawing <span class="math inline">\(3\)</span> black cards is:
<span class="math display">\[\large{\frac{{30 \choose 2}{10 \choose 3}}{{40 \choose 5}}}\]</span>
</div>
<hr />
<p>I strongly recommend students know how to figure these things out without consulting any formula sheets. I think trying to memorize this formula would be far worse than using the basics to derive it. Here is the formula.</p>
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-24" class="definition"><strong>Definition 5  (Hypergeometric Distribution)  </strong></span>We randomly select <span class="math inline">\(n\)</span> items from a population of <span class="math inline">\(N\)</span> items. Let <span class="math inline">\(r\)</span> represent the number of items from the population classified as a success, and <span class="math inline">\(k\)</span> be the number of items in the selection classified as successes. Let <span class="math inline">\(X\)</span> be the random variable representing the number of items in our selection considered successes.</p>
<span class="math display">\[P(X=k) = \large{\frac{{{N-r}\choose{n-k}}{{r}\choose{k}}}{{N \choose n}}}\]</span>
</div>
<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-25" class="example"><strong>Example 6  (Memorization Is Bad)  </strong></span>We return to our example with <span class="math inline">\(15\)</span> rocks in a bag. <span class="math inline">\(7\)</span> rocks are red. We select <span class="math inline">\(5\)</span> rocks. What is the probability of selecting exactly <span class="math inline">\(2\)</span> red rocks. We abandon critical thought and use the formula.</p>
<span class="math display">\[P(X=k) = \large{\frac{{{N-r}\choose{n-k}}{{r}\choose{k}}}{{N \choose n}}} \\
= \large{\frac{{{15-7}\choose{5-2}}{{7}\choose{2}}}{{15 \choose 7}}}\]</span>
</div>
<p>I think trying to memorize this formula is a mistake. There are also formulas for the variance and mean that are not discussed here because there is not much understanding to be gained.</p>
<div id="hypergeometric-vs.-binomial" class="section level3" number="6.3.1">
<h3 number="6.3.1"><span class="header-section-number">6.3.1</span> Hypergeometric vs. Binomial</h3>
<p>The hypergeometric distribution is closely related to the binomial distribution. We have a group of <span class="math inline">\(600\)</span> actuaries and <span class="math inline">\(400\)</span> astronauts. We select <span class="math inline">\(4\)</span> people randomly from the <span class="math inline">\(1000\)</span> to win a prize, what is probability that <span class="math inline">\(3\)</span> people are actuaries and <span class="math inline">\(1\)</span> is an astronaut?
<span class="math display">\[\large \frac{{600 \choose 3}{400 \choose 1}}{{1000 \choose 4}} = 0.3459\]</span></p>
<p>The hypergeometric distribution is different than the binomial distribution because it samples <strong>without replacement</strong>. Let’s change the problem and allow someone to win a prize multiple times. <span class="math inline">\(4\)</span> names are drawn from a hat. Each time a name is drawn a prize is given and the name is put back in the hat (this is sampling <strong>with replacement</strong>). Since <span class="math inline">\(600\)</span> of the <span class="math inline">\(1000\)</span> people are actuaries, any time we make a selection to win a prize there is a probability of <span class="math inline">\(.6\)</span> that the person is an actuary. We select <span class="math inline">\(4\)</span> people, so the distribution of prizes given to actuaries is binomial with parameters <span class="math inline">\(n = 4\)</span>, <span class="math inline">\(p = .6\)</span>. In this example the probability of <span class="math inline">\(3\)</span> actuaries winning prizes and <span class="math inline">\(1\)</span> astronaut winning a prize is:
<span class="math display">\[{4 \choose 3} \cdot .6^3 \cdot .4 = .3456\]</span></p>
<p>The hypergeometric distribution (without replacement) gives <span class="math inline">\(.3459\)</span> and the binomial distribution (with replacement) gives <span class="math inline">\(.3456\)</span>. In the hypergeometric distribution if an actuary’s name is drawn from the hat, the name does not go back in the hat. The probability that the first name drawn is an actuary is <span class="math inline">\(.6\)</span> just like our example with replacement. Once an actuary’s name is drawn from the hat the probability of drawing another actuarial name is <span class="math inline">\(\frac{599}{999} \approx.6\)</span>. This is why our hypergeometric and binomial distributions are so similar in this problem. This approximation works best when the overall population is large enough so that selecting without replacement changes the probabilities very little from selecting with replacement.</p>
</div>
</div>
<div id="poisson-distribution" class="section level2" number="6.4">
<h2 number="6.4"><span class="header-section-number">6.4</span> Poisson Distribution</h2>
<p>A Poisson distribution is appropriate when we are counting the number of times something happens in an hour or some unit of time. How many texts do I get in an hour? How often does someone in the state of California say the word “dude”. How often does a person in the city of Minneapolis trip and fall? How often does a car accident happen on interstate 35? These are things that happen at some frequency.</p>
<p>If you ever see the phrase <strong>“the average rate”</strong> you should consider that the question might be related to the Poisson distribution.</p>
<p>Often they give you the rate per hour and you need to convert it to the rate per <span class="math inline">\(2\)</span> hours, or the daily rate, or the rate per minute.
<span class="math inline">\(\lambda\)</span></p>
<hr />

<div class="example">
<span id="exm:unnamed-chunk-26" class="example"><strong>Example 7  (Poisson Scaling Rate)  </strong></span>The average rate of people spilling their coffee in the office is <span class="math inline">\(2\)</span> per hour. Generally this rate is called <span class="math inline">\(\lambda\)</span>, so <span class="math inline">\(\lambda = 2\)</span>. The rate of people spilling coffee per minute is <span class="math inline">\(\frac{\lambda}{60} = \frac{1}{30}\)</span>. The rate of people spilling their coffee every day is <span class="math inline">\(24 \lambda = 48\)</span>.
</div>
<hr />
<p>In reality most of the coffee will be spilled in the morning so the time of day changes the rate of spilling. The Poisson model does not work this way though. In the Poisson model it is assumed that the rate is constant throughout the day. The Poisson model also assumes that one person spilling their coffee doesn’t make another person spill their coffee, spills are .</p>
<hr />

<div class="theorem">
<span id="thm:unnamed-chunk-27" class="theorem"><strong>Theorem 5  (Poisson Distribution)  </strong></span>Let <span class="math inline">\(X\)</span> have a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. The probability of <span class="math inline">\(k\)</span> occurences of something happening is:
<span class="math display">\[\large P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}\]</span>
</div>
<hr />
<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-28" class="example"><strong>Example 8  (Poisson Scaling Rate)  </strong></span>Assume a Poisson model. The average rate of people spilling their coffee in the office throughout the work day is <span class="math inline">\(2.5\)</span> spills per hour. What is the probability that <span class="math inline">\(2\)</span> people spill their coffee between <span class="math inline">\(8\)</span> A.M. and <span class="math inline">\(9\)</span> A.M., and <span class="math inline">\(6\)</span> people spill their coffee between <span class="math inline">\(11\)</span> A.M. and <span class="math inline">\(1\)</span> P.M.?</p>
<p>Because the number of spills across time intervals is independent:
<span class="math display">\[P(\text{2 spills from 8-9 A.M. and 6 spills from 11 A.M.-1 P.M.}) = \\
P(\text{2 spills from 8-9 A.M.}) \cdot P(\text{6 spills from 11 A.M.-1 P.M.})\]</span></p>
<p>For the calculation from <span class="math inline">\(\text{8-9 A.M.}\)</span> we use <span class="math inline">\(\lambda = 2.5\)</span> since the time interval is <span class="math inline">\(1\)</span> hour.
<span class="math display">\[\large P(\text{2 spills from 8-9}) =\frac{e^{-2.5}2.5^2}{2!}\]</span></p>
<p>For the calculation from <span class="math inline">\(\text{11 A.M.- 1 P.M.}\)</span> we use <span class="math inline">\(\lambda = \lambda_\text{1 hour} \cdot t = 2.5 \cdot 2 = 5\)</span> since the time interval is <span class="math inline">\(2\)</span> hours.
<span class="math display">\[\large P(\text{6 spills from 11 A.M.-1 P.M.}) =\frac{e^{-5}5^6}{6!}\]</span></p>
We multiply these quantities to get the answer:
<span class="math display">\[\large \frac{e^{-2.5}2.5^2}{2!} \cdot \frac{e^{-5}5^6}{6!} = .03751\]</span>
</div>
<hr />
<p>This formula is sort of funny so we try to make some connections to things so it makes sense. People that payed attention in Calculus 2 will know that <span class="math inline">\(e^x = \frac{x^0}{0!} + \frac{x^1}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} +... = \displaystyle\sum_{n=0}^{\infty} \frac{x^n}{n!}\)</span>.</p>
<p>Let’s veryify that the sum of all probabilities is <span class="math inline">\(1\)</span> for a Poisson <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\displaystyle\sum_{n=0}^{\infty} P(X=k) = \displaystyle\sum_{n=0}^{\infty}\frac{e^{-\lambda}\lambda^k}{k!} =  \\e^{-\lambda} \displaystyle\sum_{n=0}^{\infty} \frac{\lambda^k}{k!} = e^{-\lambda}e^\lambda=1\]</span>
Using the expansion for <span class="math inline">\(e^x\)</span> it can be proven that:</p>
<hr />

<div class="theorem">
<p><span id="thm:unnamed-chunk-29" class="theorem"><strong>Theorem 6  (Poisson Distribution)  </strong></span>Let <span class="math inline">\(X\)</span> have a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.
<span class="math display">\[E(X) = \lambda\]</span>
<span class="math display">\[V(X) = \lambda\]</span></p>
Easy to remember right?
</div>
<hr />
</div>
<div id="geometric-distribution" class="section level2" number="6.5">
<h2 number="6.5"><span class="header-section-number">6.5</span> Geometric Distribution</h2>
<p>Let <span class="math inline">\(X\)</span> be the number of tries it takes to land a backflip if I land <span class="math inline">\(10%\)</span> of my backflips. This is an example of a geometric distribution. The geometric distribution is when you take independent bernoulli trials until the first success. Each backflip attempt is an independent bernoulli trial with the same probability of success (assuming I don’t improve or get tired).</p>
<p>The probability of success on my first attempt is <span class="math inline">\(.1\)</span>. The probability of success on my second attempt is <span class="math inline">\(.9 \cdot .1\)</span>, first you fail, then you succeed. The probability of success on my third attempt is <span class="math inline">\(.9 \cdot .9 \cdot .1\)</span>, first I fail twice, then I succeed. Do you see how this goes? There are failures until there is success. We call the probability of success <span class="math inline">\(p\)</span> and the probability of failure is <span class="math inline">\(1-p\)</span> but we often use <span class="math inline">\(q = 1-p\)</span> to make our formulas shorter.</p>
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-30" class="definition"><strong>Definition 6  (Geometric Distribution)  </strong></span>We perform independent Bernoulli trials, each having a probability of success <span class="math inline">\(p\)</span> and probability of failure <span class="math inline">\(q\)</span>. Let <span class="math inline">\(X\)</span> represent the trial on which the first success occurs.</p>
<span class="math display">\[P(X=k) = q^{k-1}p\]</span>
</div>
<hr />
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-31" class="definition"><strong>Definition 7  (Geometric Distribution)  </strong></span>Let <span class="math inline">\(X\)</span> be a geometric random variable with parameter p.</p>
<span class="math display">\[E(X) = \frac{1}{p}\]</span>
<span class="math display">\[V(X) = \frac{1-p}{p^2}\]</span>
</div>
<hr />
<p>There are multiple forms of the geometric distribition. So far we have been counting the attempt on which we succeed. Another way to do this is to count the number of failures before success.</p>
<p>Suppose we land a backflip on our first attempt. <span class="math inline">\(X=1\)</span> if <span class="math inline">\(X\)</span> is the try on which we succeed. If we let <span class="math inline">\(Y\)</span> be the number of failures until success, <span class="math inline">\(Y=0\)</span>. We see that there are two ways to do this.</p>
<p>I personally prefer the formulation where we use the try on which we succeed. This is because if I land <span class="math inline">\(p=\frac{1}{10}\)</span> of my backflips then the expected try on which I succeed is <span class="math inline">\(\frac{1}{p}=10\)</span>. This makes sense to me and it is pretty. If we use the other formulation where <span class="math inline">\(Y\)</span> is the number of failures before success we can say that <span class="math inline">\(Y=X-1\)</span> and
<span class="math display">\[E(Y) = E(X-1) = E(X)-1= \\ \frac{1}{p}-1 = \frac{1-p}{p}\]</span>
<span class="math inline">\(V(Y)\)</span> is unchanged because <span class="math inline">\(V(Y) = V(X+b) = V(X)\)</span>.
<span class="math display">\[V(Y) = \frac{1-p}{p^2}\]</span></p>
</div>
<div id="negative-binomial-distribution" class="section level2" number="6.6">
<h2 number="6.6"><span class="header-section-number">6.6</span> Negative Binomial Distribution</h2>
<p>The geometric distribution has a parameter <span class="math inline">\(p\)</span> and the value of the random variable is the first successful attempt. The negative binomial distribution has two parameters, <span class="math inline">\(p\)</span> and <span class="math inline">\(r\)</span>. The negative binomial distribution counts the attempt on which the <span class="math inline">\(rth\)</span> success occurs.</p>
<p>We flip an unfair coin with a probability of <span class="math inline">\(.6\)</span> for heads and <span class="math inline">\(.4\)</span> for tails. Let <span class="math inline">\(X\)</span> be the trial on which we get the <span class="math inline">\(3rd\)</span> head. We wish to know <span class="math inline">\(P(X=5)\)</span>. Let’s first consider that the <span class="math inline">\(5th\)</span> flip must be a head. Of the first <span class="math inline">\(4\)</span> flips, <span class="math inline">\(2\)</span> of them are heads. We get <span class="math inline">\(2\)</span> heads in the first four flips with probability:
<span class="math display">\[{4 \choose 2}.6^2.4^2\]</span>
We need the fifth flip to be heads so we multiply this probability by <span class="math inline">\(.6\)</span> to get our answer of:
<span class="math display">\[{4 \choose 2}.6^3.4^2\]</span>
In this case <span class="math inline">\(P(X=x) = {{x-1} \choose {r-1}}p^r(1-p)^{x-r}\)</span>.</p>
<div id="other-more-common-formulation" class="section level3" number="6.6.1">
<h3 number="6.6.1"><span class="header-section-number">6.6.1</span> Other (more common) formulation</h3>
<p>A more common way of representing this is to count the number of failures until the <span class="math inline">\(rth\)</span> success.</p>
<hr />

<div class="definition">
<span id="def:unnamed-chunk-32" class="definition"><strong>Definition 8  (Negative Binomial Distribution)  </strong></span>Let <span class="math inline">\(Y\)</span> be the number of failures until the <span class="math inline">\(rth\)</span> success. We calculate the p.d.f. of <span class="math inline">\(Y\)</span>.
<span class="math display">\[P(Y=y)={{r+y-1} \choose y}p^r(1-p)^y\]</span>
</div>
<hr />
<p>If <span class="math inline">\(X\)</span> is the R.V. representing the attempt on which the <span class="math inline">\(rth\)</span> success occurs and <span class="math inline">\(Y\)</span> is the R.V. representing the number of failures before the <span class="math inline">\(rth\)</span> success, then <span class="math inline">\(Y = X - r\)</span> because <span class="math inline">\(\text{failures = attempts - successes}\)</span>. Regardless of if the question asks you to count failures or attempts, the prepared student should be able to deduce the probability using first principles.</p>
<p>We give formulas for the expectation and variance for this formulation of the negative binomial distribution.</p>
<hr />

<div class="theorem">
<span id="thm:unnamed-chunk-33" class="theorem"><strong>Theorem 7  (Negative Binomial Distribution)  </strong></span>Let <span class="math inline">\(Y\)</span> be the number of failures until the <span class="math inline">\(rth\)</span> success.
<span class="math display">\[E(Y) = \frac{r(1-p)}{p}\]</span>
<span class="math display">\[V(Y) = \frac{r(1-p)}{p^2}\]</span>
</div>
<hr />

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="functions-of-discrete-random-variables.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-discretedistributions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
